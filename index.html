<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>COMMA: A Communicative Multimodal Multi-Agent Benchmark</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">COMMA: A Communicative Multimodal Multi-Agent Benchmark</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://tossowski.github.io/website/">Tim Ossowski</a><sup>1</sup>,</span>
            <span class="author-block">
              <span class="author-block">
              <a href="https://www.linkedin.com/in/danyal-maqbool-b93408215/">Danyal Maqbool</a><sup>1</sup>,
            </span>
              <a href="https://chenjix.github.io/">Jixuan Chen</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=eyIrttAAAAAJ&hl=en">Zefan Cai</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://ml4mi.wisc.edu/staff/bradshaw-tyler/">Tyler Bradshaw</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://junjiehu.github.io/">Junjie Hu</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Wisconsin-Madison,</span>
            <span class="author-block"><sup>2</sup>Nanjing University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2410.07553"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/tossowski/COMMA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/comma_demo.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">COMMA</span> is a multimodal benchmark designed to assess the collaborative abilities of multimodal agents. Our benchmark is inspired by the cooperative gameplay scenario in the <a href="https://keeptalkinggame.com/">Keep Talking and Nobody Explodes</a> Game. In this game, two players work together to defuse a bomb under time pressure.
        One agent, the defuser, can see the bomb but lacks the instructions to disarm it. The other agent,
        the expert, has access to the bomb's manual but cannot see the bomb itself. The agents must rely
        on effective communication to exchange information, navigate challenges, and defuse the bomb.
      </h2>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      
      <p>
        Overview of the interaction between the Solver and Expert agents in our benchmark. Both agents operate with structured input corresponding to working and episodic memory. The Solver receives an image of the puzzle state (working memory) and makes decisions based on the available actions described in the task prompt. The Expert, guided by instruction manuals (working memory), provides advice based on the Solver's descriptions, such as indicating which buttons to press. The Solver can choose to execute actions by interacting with the environment or communicate with the Expert for further guidance. Their interaction is documented through a dialogue, showcasing the cooperation required to complete the task. Both agents engage in self-reflection by referencing the conversation history, which is continuously updated and incorporated into their input as episodic memory.
      </p>
    </div>
  </div>
</section> -->



<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-shiba">
          <img src="./static/images/atmpuzzle.png"
                 class="interpolation-image"
                 alt="ATM Puzzle."/>
        </div>
        <div class="item item-chair-tp">
          <img src="./static/images/telehealth.png"
                 class="interpolation-image"
                 alt="Telehealth Puzzle."/>
        </div>
        <div class="item item-steve">
          <img src="./static/images/keypad.png"
                 class="interpolation-image"
                 alt="Keypad Puzzle."/>
        </div>
        <div class="item item-shiba">
          <img src="./static/images/password.png"
                 class="interpolation-image"
                 alt="Password Puzzle."/>
        </div>
        <div class="item item-shiba">
          <img src="./static/images/wires.png"
                 class="interpolation-image"
                 alt="Wires Puzzle."/>
        </div>
        <div class="item item-shiba">
          <img src="./static/images/memory.png"
                 class="interpolation-image"
                 alt="Memory Puzzle."/>
        </div>
        <div class="item item-shiba">
          <img src="./static/images/maze.png"
                 class="interpolation-image"
                 alt="Maze Puzzle."/>
        </div>
        <div class="item item-shiba">
          <img src="./static/images/led.png"
                 class="interpolation-image"
                 alt="LED Puzzle."/>
        </div>
        <div class="item item-shiba">
          <img src="./static/images/who.png"
                 class="interpolation-image"
                 alt="Who Puzzle."/>
        </div>
        <div class="item item-shiba">
          <img src="./static/images/color.png"
                 class="interpolation-image"
                 alt="Color Puzzle."/>
        </div>
       
      </div>
      <p>
        Example puzzles presented to the solver in our benchmark. Notably, each puzzle is multimodal and cannot be solved without communicating with the expert.
      </p>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce a novel benchmark designed to evaluate the collaborative performance of multimodal multi-agent systems through language communication. Our benchmark
features a variety of puzzles, providing a comprehensive evaluation across four
key categories of agentic capability in a communicative collaboration setting. By
testing both agent-agent and agent-human collaborations using open-source and
closed-source models, our findings reveal surprising weaknesses in state-of-theart models, including proprietary models like GPT-4o. These models struggle to
outperform even a simple random agent baseline in agent-agent collaboration and
only surpass the random baseline when a human is involved.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      
      <div class="column">
        <h2 class="title is-3">Agent Interaction</h2>
        <img width="100%" src="./static/images/agents.jpg"
                 class="interpolation-image"
                 alt="Agent Setup"/>
        <div class="content has-text-justified">
          <p>
            Overview of the interaction between the Solver and Expert agents in our benchmark. Both agents operate with structured input corresponding to working and episodic memory. The Solver receives an image of the puzzle state (working memory) and makes decisions based on the available actions described in the task prompt. The Expert, guided by instruction manuals (working memory), provides advice based on the Solver's descriptions, such as indicating which buttons to press. The Solver can choose to execute actions by interacting with the environment or communicate with the Expert for further guidance. Their interaction is documented through a dialogue, showcasing the cooperation required to complete the task. Both agents engage in self-reflection by referencing the conversation history, which is continuously updated and incorporated into their input as episodic memory.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <div class="container is-max-desktop">
      
      <div class="columns is-centered has-text-centered">
        
        <div class="column">
          <h2 class="title is-3">Results</h2>
          <hr class="section-divider"/>
          <h3 class="title is-4 has-text-left">Conversation Length vs Success Rate</h3>

          <img width="60%" src="./static/images/graph.png"
                class="interpolation-image",
                alt="Graph of Conversation Length and Success Rate."/>
          
          <div class="content has-text-justified">

            
            <p>
             We find that even the most powerful closed-source LLMs struggle to communicate without human involvement. Here we plot the success rate as a function of conversation length for different settings. The open-source models such as LLaVA, InternVL, actually underperform the random baseline for most conversation lengths, suggesting limited ability to utilize episodic memory and plateau after about 4-5 conversation turns.
            </p>
          <hr class="section-divider"/>
          <h3 class="title is-4 has-text-left">Qualitative Error Analysis</h3>
          <img width="90%" src="./static/images/error_analysis.png"
                 class="interpolation-image"
                 alt="Pie chart of common failure modes."/>
            <p>
             We further analyze the underlying reason for failure based on conversations of all 1000 puzzles. We look at the best closed-source and open-source performing model conversations: GPT-4o (left) and LLaMA 3.2 (right), and define the following failure modes:
             <ul>
              <li><b>Repetition Loop:</b> The solver repeats its past incorrect actions, even if is in a situation
                it has encountered before.</li>
              <li><b>Misinterpretation:</b> The solver misunderstands the current puzzle state/signal due to poor visual grounding or lack of situational awareness, resulting in failure.</li>
              <li><b>Roleplay:</b> The expert thinks it is the solver or vice versa, despite the prompt assigning it a role.</li>
              <li><b>Miscommunication:</b> The solver agent occasionally does not listen to the expert's
                instructions, attempting to solve the puzzle on its own as if it were the expert.</li>
          </ul> 
          For more detailed examples and results, feel free to check out <a href="https://arxiv.org/pdf/2410.07553">our paper!</a>
            </p>
          <hr class="section-divider"/>
          <h3 class="title is-4 has-text-left">Communication Efficiency</h3>
          <div class="has-text-centered">
              
              <img width="70%" src="./static/images/efficiency.png"
                  class="interpolation-image"
                  alt="Efficiency scores of models."/>
            </div>

            
            <p>
             We observe a tradeoff between performance and token usage on our benchmark. Humans are able to quickly and reliably solve puzzles with just a few sentences of communication. In the graph, agents in the top left corner achieve a better balance between performance and efficiency. Among AI agents, GPT-4o performs best, but still far below human efficiency. The size of each point is proportional to the model's communication efficiency score, defined below:
            </p>
            
            <div class="has-text-centered">

              <img width="70%" src="./static/images/equation.png"
                  class="interpolation-image"
                  alt="Efficiency scores of models."/>
            </div>
            <p>
             We expect that hybrid thinking approaches, which better balance internal reasoning and external communication, will be key to closing the efficiency gap between current models and human reasoning.
            </p>
          <hr class="section-divider"/>

          </div>
        </div>
      </div>

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3">Leaderboard üèÜ</h2>
        <img src="./static/images/table.png"
                class="interpolation-image",
                alt="Table of performance."/>
        <p>
          Using COMMA, we benchmark the collaborative capabilities of closed-source and open-source multimodal LLMs, summarized in the table above.
        </p>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">


    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Work</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work related to multimodal agents which inspired ours.
          </p>
          <p>
            <a href="https://arxiv.org/pdf/2401.13649">Visual Web Arena</a> introduces the idea of using multimodal agents to perform complex multi-step web tasks in a controlled environment. We are inspired by their environment, and extend their framework to involve multi-agent collaboration.
          </p>
          <p>
            Alane Suhr has published many related works on the subject of using LLMs decision-making agents such as <a href="https://arxiv.org/pdf/2405.10292">Fine-Tuning Large Vision-Language Models as
              Decision-Making Agents via Reinforcement Learning</a> and <a href="https://arxiv.org/pdf/2306.00924">Minding Language Models' (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker</a>. Her works strongly inspired the ideas in our benchmark.
          </p>
          <p>
            Some works illustrate the potenial of agents to collaborate to perform difficult tasks such as <a href="https://arxiv.org/abs/2307.07924">Software Development</a> and <a href=" https://arxiv.org/abs/2406.08979">Function Generation</a>.
          </p>
          <p>
            For a more comprehensive list, feel free to check out <a href="https://arxiv.org/abs/2402.15116">this survey paper</a>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{ossowski2024comma,
      title={COMMA: A Communicative Multimodal Multi-Agent Benchmark},
      author={Ossowski, Timothy and Chen, Jixuan and Maqbool, Danyal and Cai, Zefan and Bradshaw, Tyler and Hu, Junjie},
      journal={arXiv preprint arXiv:2410.07553},
      year={2024}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2410.07553">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/tossowski/AgentBenchmark" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
